{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "import torch\n",
    "from torchtune.utils.profiling_utils import (\n",
    "    _DEFAULT_PROFILER_ACTIVITIES,\n",
    "    _DEFAULT_SCHEDULE_CFG,\n",
    "    _DEFAULT_PROFILER_OPTS,\n",
    "    _ExperimentalConfig,\n",
    ")\n",
    "from torchtune import config\n",
    "from tests.recipes.utils import dummy_alpaca_dataset_config\n",
    "import os\n",
    "\n",
    "config_file = \"./custom_configs/7B_lora.yaml\"\n",
    "fixtures_dir = \"tests/assets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.profiler.profiler.profile at 0x7f2d620a00d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.instantiate(\n",
    "    OmegaConf.create(\n",
    "        {\"_component_\": \"torch.profiler.profile\", **_DEFAULT_PROFILER_OPTS}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_cfg = OmegaConf.from_dotlist(dummy_alpaca_dataset_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.modules.tokenizers import SentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceTokenizer(os.path.join(fixtures_dir, \"m.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100 examples [00:00, 1663.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = config.instantiate(alpaca_cfg.dataset, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests.recipes.utils as test_utils\n",
    "from tests.recipes.utils import MODEL_TEST_CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model._component_=torchtune.models.llama2.lora_llama2',\n",
       " \"model.lora_attn_modules=['q_proj', 'k_proj', 'v_proj', 'output_proj']\",\n",
       " 'model.apply_lora_to_mlp=True',\n",
       " 'model.apply_lora_to_output=False',\n",
       " 'model.vocab_size=32000',\n",
       " 'model.num_layers=4',\n",
       " 'model.num_heads=16',\n",
       " 'model.embed_dim=256',\n",
       " 'model.max_seq_len=2048',\n",
       " 'model.norm_eps=1e-5',\n",
       " 'model.num_kv_heads=8',\n",
       " 'model.lora_rank=8',\n",
       " 'model.lora_alpha=16',\n",
       " 'model.lora_dropout=0.0',\n",
       " 'model.quantize_base=True']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_qlora_cfg = MODEL_TEST_CONFIGS[\"llama2_qlora\"]\n",
    "llama2_qlora_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'_component_': 'torchtune.models.llama2.lora_llama2', 'lora_attn_modules': ['q_proj', 'k_proj', 'v_proj', 'output_proj'], 'apply_lora_to_mlp': True, 'apply_lora_to_output': False, 'vocab_size': 32000, 'num_layers': 4, 'num_heads': 16, 'embed_dim': 256, 'max_seq_len': 2048, 'norm_eps': 1e-05, 'num_kv_heads': 8, 'lora_rank': 8, 'lora_alpha': 16, 'lora_dropout': 0.0, 'quantize_base': True}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg = OmegaConf.from_dotlist(llama2_qlora_cfg)\n",
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_qlora = config.instantiate(model_cfg.model)\n",
    "torch.save(llama2_qlora.state_dict(), \"./llama2_qlora.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = torch.load(\"./llama2_qlora.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['tok_embeddings.weight', 'layers.0.sa_norm.scale', 'layers.0.attn.q_proj.weight', 'layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.k_proj.weight', 'layers.0.attn.k_proj.lora_a.weight', 'layers.0.attn.k_proj.lora_b.weight', 'layers.0.attn.v_proj.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.0.attn.output_proj.weight', 'layers.0.attn.output_proj.lora_a.weight', 'layers.0.attn.output_proj.lora_b.weight', 'layers.0.mlp_norm.scale', 'layers.0.mlp.w1.weight', 'layers.0.mlp.w1.lora_a.weight', 'layers.0.mlp.w1.lora_b.weight', 'layers.0.mlp.w2.weight', 'layers.0.mlp.w2.lora_a.weight', 'layers.0.mlp.w2.lora_b.weight', 'layers.0.mlp.w3.weight', 'layers.0.mlp.w3.lora_a.weight', 'layers.0.mlp.w3.lora_b.weight', 'layers.1.sa_norm.scale', 'layers.1.attn.q_proj.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.k_proj.weight', 'layers.1.attn.k_proj.lora_a.weight', 'layers.1.attn.k_proj.lora_b.weight', 'layers.1.attn.v_proj.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.1.attn.output_proj.weight', 'layers.1.attn.output_proj.lora_a.weight', 'layers.1.attn.output_proj.lora_b.weight', 'layers.1.mlp_norm.scale', 'layers.1.mlp.w1.weight', 'layers.1.mlp.w1.lora_a.weight', 'layers.1.mlp.w1.lora_b.weight', 'layers.1.mlp.w2.weight', 'layers.1.mlp.w2.lora_a.weight', 'layers.1.mlp.w2.lora_b.weight', 'layers.1.mlp.w3.weight', 'layers.1.mlp.w3.lora_a.weight', 'layers.1.mlp.w3.lora_b.weight', 'layers.2.sa_norm.scale', 'layers.2.attn.q_proj.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.k_proj.weight', 'layers.2.attn.k_proj.lora_a.weight', 'layers.2.attn.k_proj.lora_b.weight', 'layers.2.attn.v_proj.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.2.attn.output_proj.weight', 'layers.2.attn.output_proj.lora_a.weight', 'layers.2.attn.output_proj.lora_b.weight', 'layers.2.mlp_norm.scale', 'layers.2.mlp.w1.weight', 'layers.2.mlp.w1.lora_a.weight', 'layers.2.mlp.w1.lora_b.weight', 'layers.2.mlp.w2.weight', 'layers.2.mlp.w2.lora_a.weight', 'layers.2.mlp.w2.lora_b.weight', 'layers.2.mlp.w3.weight', 'layers.2.mlp.w3.lora_a.weight', 'layers.2.mlp.w3.lora_b.weight', 'layers.3.sa_norm.scale', 'layers.3.attn.q_proj.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.k_proj.weight', 'layers.3.attn.k_proj.lora_a.weight', 'layers.3.attn.k_proj.lora_b.weight', 'layers.3.attn.v_proj.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.3.attn.output_proj.weight', 'layers.3.attn.output_proj.lora_a.weight', 'layers.3.attn.output_proj.lora_b.weight', 'layers.3.mlp_norm.scale', 'layers.3.mlp.w1.weight', 'layers.3.mlp.w1.lora_a.weight', 'layers.3.mlp.w1.lora_b.weight', 'layers.3.mlp.w2.weight', 'layers.3.mlp.w2.lora_a.weight', 'layers.3.mlp.w2.lora_b.weight', 'layers.3.mlp.w3.weight', 'layers.3.mlp.w3.lora_a.weight', 'layers.3.mlp.w3.lora_b.weight', 'norm.scale', 'output.weight'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in llama2_qlora.parameters()) / 1e6\n",
    "llama2_qlora.output.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = _ExperimentalConfig(verbose=True)\n",
    "e2 = _ExperimentalConfig(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56, 56)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(e1), sys.getsizeof(e2), sys.getsizeof(_ExperimentalConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.profiler.profiler.schedule.<locals>.schedule_fn(step: int) -> torch.profiler.profiler.ProfilerAction>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.instantiate(OmegaConf.create(DEFAULT_SCHEDULE_CFG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_component_': 'torch.profiler.schedule', 'wait': 10, 'warmup': 5, 'active': 3, 'repeat': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DictConfig(DEFAULT_SCHEDULE_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = \"\"\"\n",
    "profile:\n",
    "  enabled: True\n",
    "  CPU: True\n",
    "  CUDA: True\n",
    "  #output_dir: ${artifact_dir}/profiling\n",
    "  #torch.profiler.profile\n",
    "  profiler:\n",
    "    _component_: torch.profiler.profile\n",
    "    profile_memory: False\n",
    "    with_stack: True\n",
    "    record_shapes: False\n",
    "    with_flops: True\n",
    "  #torch.profiler.schedule\n",
    "  schedule:\n",
    "    _component_: torch.profiler.schedule\n",
    "    wait: 3\n",
    "    warmup: 1\n",
    "    active: 1\n",
    "    repeat: 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = OmegaConf.create(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s.profile.pop(\"schedule\")\n",
    "s.profile.pop(\"enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = cfg.pop(\"profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigKeyError",
     "evalue": "Key not found: 'hello'\n    full_key: hello\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigKeyError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/omegaconf/dictconfig.py:517\u001b[0m, in \u001b[0;36mDictConfig.pop\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_and_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/omegaconf/base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[0;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[43mformat_and_raise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcause\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtype_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/omegaconf/_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[1;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[0;32m--> 899\u001b[0m \u001b[43m_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/omegaconf/_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/omegaconf/dictconfig.py:515\u001b[0m, in \u001b[0;36mDictConfig.pop\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\n\u001b[1;32m    512\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (path: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m                 )\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me)\n",
      "\u001b[0;31mConfigKeyError\u001b[0m: Key not found: 'hello'\n    full_key: hello\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "cfg.pop(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtune.config._instantiate import instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_cfg = cfg.profile.schedule\n",
    "scheduler_cfg.repeat = 0\n",
    "\n",
    "default_scheduler = {\n",
    "    \"_component_\": \"torch.profiler.schedule\",\n",
    "    \"wait\": 1,\n",
    "    \"warmup\": 1,\n",
    "    \"active\": 1,\n",
    "    \"repeat\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_component_': 'torch.profiler.schedule', 'wait': 1, 'warmup': 1, 'active': 1, 'repeat': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "OmegaConf.create(default_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_component_': 'torch.profiler.schedule', 'wait': 1, 'warmup': 1, 'active': 1, 'repeat': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OmegaConf.select(cfg.profile, \"schedule\", default=None, throw_on_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schedule = instantiate(scheduler_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_schedule = torch.profiler.schedule(wait=1, warmup=1, active=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actions = [test_schedule(i) for i in range(5)]\n",
    "ref_actions = [ref_schedule(i) for i in range(5)]\n",
    "assert test_actions == ref_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_component_': 'torch.profiler.profile', 'record_shapes': True, 'profile_memory': True, 'with_stack': True, 'with_flops': True}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = OmegaConf.load(config_file)\n",
    "cfg.profile.profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.profile.CUDA\n",
    "OmegaConf.select(cfg.profile, \"CPU\", default=True, throw_on_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = instantiate(\n",
    "    cfg.profile.profiler,\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU if cfg.profile.CPU else None,\n",
    "        torch.profiler.ProfilerActivity.CUDA if cfg.profile.CUDA else None,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.schedule = test_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.with_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ProfilerAction.NONE: 0>,\n",
       " <ProfilerAction.WARMUP: 1>,\n",
       " <ProfilerAction.RECORD_AND_SAVE: 3>,\n",
       " <ProfilerAction.NONE: 0>,\n",
       " <ProfilerAction.WARMUP: 1>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.schedule(i) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    a in p.activities\n",
    "    for a in [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_prof = torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    "    with_flops=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert p.activities == ref_prof.activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_llama_config = {\n",
    "    \"architectures\": [\"LLaMAForCausalLM\"],\n",
    "    \"bos_token_id\": 0,\n",
    "    \"eos_token_id\": 1,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 128,\n",
    "    \"intermediate_size\": 352,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"max_sequence_length\": 1024,\n",
    "    \"model_type\": \"llama\",\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"pad_token_id\": -1,\n",
    "    \"rms_norm_eps\": 1e-06,\n",
    "    \"transformers_version\": \"4.28.1\",\n",
    "    \"use_cache\": True,\n",
    "    \"vocab_size\": 32000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama import LlamaConfig\n",
    "from transformers import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_config = LlamaConfig(**small_llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:494: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pas_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM(config=llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_component_': 'torchtune.utils.FullModelHFCheckpointer', 'checkpoint_dir': '/home/ubuntu/model_checkpoints/tiny_llama', 'checkpoint_files': ['model.safetensors'], 'recipe_checkpoint': None, 'output_dir': '${.checkpoint_dir}/trained', 'model_type': 'LLAMA2', 'resume_from_checkpoint': False}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_llama_cfg = OmegaConf.load(\"./custom_configs/tiny_llama.yaml\").checkpointer\n",
    "tiny_llama_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.resolve(tiny_llama_cfg)\n",
    "ckptr = config.instantiate(tiny_llama_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(tiny_llama_cfg.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = p / tiny_llama_cfg.checkpoint_files[0]\n",
    "\n",
    "model_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:494: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pas_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaConfig' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/envs/torchtune/lib/python3.11/site-packages/transformers/configuration_utils.py:264\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    263\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaConfig' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "m.config.pad_token_id = 0\n",
    "m.config.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.recipes.utils import MODEL_TEST_CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model._component_=torchtune.models.llama2.lora_llama2',\n",
       " \"model.lora_attn_modules=['q_proj', 'k_proj', 'v_proj', 'output_proj']\",\n",
       " 'model.apply_lora_to_mlp=True',\n",
       " 'model.apply_lora_to_output=False',\n",
       " 'model.vocab_size=32000',\n",
       " 'model.num_layers=4',\n",
       " 'model.num_heads=16',\n",
       " 'model.embed_dim=256',\n",
       " 'model.max_seq_len=2048',\n",
       " 'model.norm_eps=1e-5',\n",
       " 'model.num_kv_heads=8',\n",
       " 'model.lora_rank=8',\n",
       " 'model.lora_alpha=16',\n",
       " 'model.lora_dropout=0.0',\n",
       " 'model.quantize_base=True']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_TEST_CONFIGS[\"llama2_qlora\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.utils import FullModelTorchTuneCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No file with name: model.safetensors found in /home/ubuntu/model_checkpoints/tiny_llama.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chkptr \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiny_llama_cfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torchtune/torchtune/config/_instantiate.py:106\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Resolve all interpolations, or references to other fields within the same config\u001b[39;00m\n\u001b[1;32m    104\u001b[0m OmegaConf\u001b[38;5;241m.\u001b[39mresolve(config)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_instantiate_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torchtune/torchtune/config/_instantiate.py:31\u001b[0m, in \u001b[0;36m_instantiate_node\u001b[0;34m(node, *args)\u001b[0m\n\u001b[1;32m     29\u001b[0m     _component_ \u001b[38;5;241m=\u001b[39m _get_component_from_path(node\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_component_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     30\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_component_\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_component_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InstantiationError(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot instantiate specified object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve specified a _component_ field with a valid dotpath.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/torchtune/torchtune/config/_instantiate.py:20\u001b[0m, in \u001b[0;36m_create_component\u001b[0;34m(_component_, args, kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_component\u001b[39m(\n\u001b[1;32m     16\u001b[0m     _component_: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[1;32m     17\u001b[0m     args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m     18\u001b[0m     kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     19\u001b[0m ):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_component_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torchtune/torchtune/utils/_checkpointing/_checkpointer.py:302\u001b[0m, in \u001b[0;36mFullModelHFCheckpointer.__init__\u001b[0;34m(self, checkpoint_dir, checkpoint_files, model_type, output_dir, adapter_checkpoint, recipe_checkpoint, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    293\u001b[0m     checkpoint_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     resume_from_checkpoint: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_dir \u001b[38;5;241m=\u001b[39m Path(checkpoint_dir)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_hf_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter_checkpoint \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    304\u001b[0m         get_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_dir, adapter_checkpoint)\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m adapter_checkpoint\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_type \u001b[38;5;241m=\u001b[39m ModelType[model_type]\n",
      "File \u001b[0;32m~/torchtune/torchtune/utils/_checkpointing/_checkpointer.py:342\u001b[0m, in \u001b[0;36mFullModelHFCheckpointer._validate_hf_checkpoint_files\u001b[0;34m(self, checkpoint_files)\u001b[0m\n\u001b[1;32m    340\u001b[0m checkpoint_paths: List[Path] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m checkpoint_files:\n\u001b[0;32m--> 342\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     checkpoint_paths\u001b[38;5;241m.\u001b[39mappend(checkpoint_path)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(checkpoint_paths)\n",
      "File \u001b[0;32m~/torchtune/torchtune/utils/_checkpointing/_checkpointer_utils.py:52\u001b[0m, in \u001b[0;36mget_path\u001b[0;34m(input_dir, filename, missing_ok)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# If missing_ok is False, raise an error if the path is invalid\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_ok \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file with name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[0;31mValueError\u001b[0m: No file with name: model.safetensors found in /home/ubuntu/model_checkpoints/tiny_llama."
     ]
    }
   ],
   "source": [
    "chkptr = config.instantiate(tiny_llama_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
